"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[498],{3466(n,e,i){i.r(e),i.d(e,{assets:()=>d,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"vla","title":"Chapter 5: Vision-Language-Action Models for Humanoid Control","description":"Learning Objectives","source":"@site/docs/05-vla.md","sourceDirName":".","slug":"/vla","permalink":"/vla","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 4: NVIDIA Isaac for Humanoid Development","permalink":"/nvidia-isaac"},"next":{"title":"Chapter 6: Capstone Project - Building an Autonomous Humanoid System","permalink":"/capstone"}}');var t=i(4848),s=i(8453);const a={},r="Chapter 5: Vision-Language-Action Models for Humanoid Control",d={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"5.1 Introduction to VLA Models",id:"51-introduction-to-vla-models",level:2},{value:"What are Vision-Language-Action Models?",id:"what-are-vision-language-action-models",level:3},{value:"Why VLA for Humanoids?",id:"why-vla-for-humanoids",level:3},{value:"5.2 Vision Encoders for Robotics",id:"52-vision-encoders-for-robotics",level:2},{value:"Pre-trained Vision Models",id:"pre-trained-vision-models",level:3},{value:"Multi-View Fusion",id:"multi-view-fusion",level:3},{value:"5.3 Language Understanding",id:"53-language-understanding",level:2},{value:"Instruction Encoding",id:"instruction-encoding",level:3},{value:"Grounding Language to Vision",id:"grounding-language-to-vision",level:3},{value:"5.4 Action Prediction",id:"54-action-prediction",level:2},{value:"Action Tokenization",id:"action-tokenization",level:3},{value:"Autoregressive Action Decoder",id:"autoregressive-action-decoder",level:3},{value:"5.5 Complete VLA Model",id:"55-complete-vla-model",level:2},{value:"OpenVLA Architecture",id:"openvla-architecture",level:3},{value:"5.6 Training VLA Models",id:"56-training-vla-models",level:2},{value:"Data Collection",id:"data-collection",level:3},{value:"Training Loop",id:"training-loop",level:3},{value:"5.7 Deployment on Humanoids",id:"57-deployment-on-humanoids",level:2},{value:"ROS 2 VLA Node",id:"ros-2-vla-node",level:3},{value:"5.8 Advanced Topics",id:"58-advanced-topics",level:2},{value:"Chain-of-Thought Reasoning",id:"chain-of-thought-reasoning",level:3},{value:"Multi-Task Learning",id:"multi-task-learning",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Lab: Building a Mini-VLA",id:"lab-building-a-mini-vla",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-5-vision-language-action-models-for-humanoid-control",children:"Chapter 5: Vision-Language-Action Models for Humanoid Control"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the VLA architecture and its components"}),"\n",(0,t.jsx)(e.li,{children:"Implement vision-language models for robot perception"}),"\n",(0,t.jsx)(e.li,{children:"Design action prediction networks for manipulation"}),"\n",(0,t.jsx)(e.li,{children:"Train VLA models on robot demonstration data"}),"\n",(0,t.jsx)(e.li,{children:"Deploy VLA systems on humanoid robots"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"51-introduction-to-vla-models",children:"5.1 Introduction to VLA Models"}),"\n",(0,t.jsx)(e.h3,{id:"what-are-vision-language-action-models",children:"What are Vision-Language-Action Models?"}),"\n",(0,t.jsx)(e.p,{children:"VLA models combine three modalities:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision"}),": Understanding visual scenes through images/video"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language"}),": Processing natural language instructions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action"}),": Generating robot control commands"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    VLA Architecture                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502  Image  \u2502   \u2502  Text   \u2502   \u2502     Action Tokens       \u2502   \u2502\n\u2502   \u2502 Encoder \u2502   \u2502 Encoder \u2502   \u2502   (x, y, z, rx, ry, rz) \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502        \u2502             \u2502                      \u2502                 \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502                 \u2502\n\u2502               \u25bc                             \u2502                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502                 \u2502\n\u2502   \u2502  Multimodal Transformer \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502   \u2502   (Cross-Attention)     \u2502                                 \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n\u2502               \u2502                                               \u2502\n\u2502               \u25bc                                               \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                 \u2502\n\u2502   \u2502    Action Decoder       \u2502                                 \u2502\n\u2502   \u2502  (7-DOF End-Effector)   \u2502                                 \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n\u2502                                                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h3,{id:"why-vla-for-humanoids",children:"Why VLA for Humanoids?"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Challenge"}),(0,t.jsx)(e.th,{children:"VLA Solution"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Complex instructions"}),(0,t.jsx)(e.td,{children:"Language understanding"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Scene understanding"}),(0,t.jsx)(e.td,{children:"Visual perception"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Skill generalization"}),(0,t.jsx)(e.td,{children:"Foundation model transfer"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"Multi-step tasks"}),(0,t.jsx)(e.td,{children:"Temporal reasoning"})]})]})]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"52-vision-encoders-for-robotics",children:"5.2 Vision Encoders for Robotics"}),"\n",(0,t.jsx)(e.h3,{id:"pre-trained-vision-models",children:"Pre-trained Vision Models"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom transformers import ViTModel, ViTConfig\n\nclass RobotVisionEncoder(nn.Module):\n    """Vision encoder for robot observations."""\n\n    def __init__(self, pretrained: str = "google/vit-base-patch16-224"):\n        super().__init__()\n\n        # Load pre-trained ViT\n        self.vit = ViTModel.from_pretrained(pretrained)\n        self.hidden_dim = self.vit.config.hidden_size\n\n        # Freeze early layers\n        for param in self.vit.embeddings.parameters():\n            param.requires_grad = False\n\n        # Projection for robot features\n        self.projection = nn.Linear(self.hidden_dim, 512)\n\n    def forward(self, images: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            images: (B, 3, 224, 224) RGB images\n        Returns:\n            features: (B, 197, 512) patch features\n        """\n        outputs = self.vit(pixel_values=images)\n        features = outputs.last_hidden_state\n        return self.projection(features)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"multi-view-fusion",children:"Multi-View Fusion"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class MultiViewEncoder(nn.Module):\n    """Encode multiple camera views for humanoid perception."""\n\n    def __init__(self, num_views: int = 3):\n        super().__init__()\n        self.num_views = num_views\n\n        # Shared vision encoder\n        self.vision_encoder = RobotVisionEncoder()\n\n        # View position embeddings\n        self.view_embeddings = nn.Embedding(num_views, 512)\n\n        # Cross-view attention\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=512,\n            num_heads=8,\n            batch_first=True\n        )\n\n    def forward(self, views: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            views: (B, num_views, 3, 224, 224)\n        Returns:\n            fused_features: (B, seq_len, 512)\n        """\n        B = views.shape[0]\n\n        # Encode each view\n        all_features = []\n        for i in range(self.num_views):\n            features = self.vision_encoder(views[:, i])\n            view_emb = self.view_embeddings(torch.tensor(i, device=views.device))\n            features = features + view_emb\n            all_features.append(features)\n\n        # Concatenate views\n        all_features = torch.cat(all_features, dim=1)\n\n        # Cross-view attention\n        fused, _ = self.cross_attention(\n            all_features, all_features, all_features\n        )\n\n        return fused\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"53-language-understanding",children:"5.3 Language Understanding"}),"\n",(0,t.jsx)(e.h3,{id:"instruction-encoding",children:"Instruction Encoding"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from transformers import T5EncoderModel, T5Tokenizer\n\nclass InstructionEncoder(nn.Module):\n    """Encode natural language instructions."""\n\n    def __init__(self, model_name: str = "t5-base"):\n        super().__init__()\n\n        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n        self.encoder = T5EncoderModel.from_pretrained(model_name)\n\n        # Project to common dimension\n        self.projection = nn.Linear(768, 512)\n\n    def forward(self, instructions: list[str]) -> torch.Tensor:\n        """\n        Args:\n            instructions: List of instruction strings\n        Returns:\n            features: (B, seq_len, 512)\n        """\n        # Tokenize\n        tokens = self.tokenizer(\n            instructions,\n            return_tensors="pt",\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n\n        # Encode\n        outputs = self.encoder(\n            input_ids=tokens.input_ids.to(self.encoder.device),\n            attention_mask=tokens.attention_mask.to(self.encoder.device)\n        )\n\n        return self.projection(outputs.last_hidden_state)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"grounding-language-to-vision",children:"Grounding Language to Vision"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class VisionLanguageGrounding(nn.Module):\n    """Ground language instructions to visual features."""\n\n    def __init__(self, hidden_dim: int = 512):\n        super().__init__()\n\n        # Cross-modal attention\n        self.cross_attention = nn.ModuleList([\n            nn.MultiheadAttention(hidden_dim, 8, batch_first=True)\n            for _ in range(4)\n        ])\n\n        self.ffn = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim * 4),\n            nn.GELU(),\n            nn.Linear(hidden_dim * 4, hidden_dim)\n        )\n\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        self.norm2 = nn.LayerNorm(hidden_dim)\n\n    def forward(\n        self,\n        vision_features: torch.Tensor,\n        language_features: torch.Tensor\n    ) -> torch.Tensor:\n        """\n        Args:\n            vision_features: (B, V, 512) visual patch features\n            language_features: (B, L, 512) language token features\n        Returns:\n            grounded_features: (B, V, 512)\n        """\n        x = vision_features\n\n        for attn in self.cross_attention:\n            # Language attends to vision\n            attended, _ = attn(language_features, x, x)\n\n            # Vision attends to language\n            x_new, _ = attn(x, attended, attended)\n            x = self.norm1(x + x_new)\n            x = self.norm2(x + self.ffn(x))\n\n        return x\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"54-action-prediction",children:"5.4 Action Prediction"}),"\n",(0,t.jsx)(e.h3,{id:"action-tokenization",children:"Action Tokenization"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class ActionTokenizer:\n    """Convert continuous actions to discrete tokens."""\n\n    def __init__(\n        self,\n        num_bins: int = 256,\n        action_dim: int = 7  # 6 DOF + gripper\n    ):\n        self.num_bins = num_bins\n        self.action_dim = action_dim\n\n        # Action ranges for each dimension\n        self.ranges = {\n            \'x\': (-0.5, 0.5),      # Position (meters)\n            \'y\': (-0.5, 0.5),\n            \'z\': (0.0, 1.0),\n            \'rx\': (-3.14, 3.14),   # Rotation (radians)\n            \'ry\': (-3.14, 3.14),\n            \'rz\': (-3.14, 3.14),\n            \'gripper\': (0.0, 1.0)  # Gripper open/close\n        }\n\n    def encode(self, actions: torch.Tensor) -> torch.Tensor:\n        """\n        Args:\n            actions: (B, T, 7) continuous actions\n        Returns:\n            tokens: (B, T, 7) discrete action tokens\n        """\n        tokens = []\n        for i, (name, (low, high)) in enumerate(self.ranges.items()):\n            # Normalize to [0, 1]\n            normalized = (actions[..., i] - low) / (high - low)\n            normalized = torch.clamp(normalized, 0, 1)\n\n            # Discretize\n            token = (normalized * (self.num_bins - 1)).long()\n            tokens.append(token)\n\n        return torch.stack(tokens, dim=-1)\n\n    def decode(self, tokens: torch.Tensor) -> torch.Tensor:\n        """Convert tokens back to continuous actions."""\n        actions = []\n        for i, (name, (low, high)) in enumerate(self.ranges.items()):\n            normalized = tokens[..., i].float() / (self.num_bins - 1)\n            action = normalized * (high - low) + low\n            actions.append(action)\n\n        return torch.stack(actions, dim=-1)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"autoregressive-action-decoder",children:"Autoregressive Action Decoder"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class ActionDecoder(nn.Module):\n    """Decode actions autoregressively."""\n\n    def __init__(\n        self,\n        hidden_dim: int = 512,\n        action_dim: int = 7,\n        num_bins: int = 256,\n        max_steps: int = 10\n    ):\n        super().__init__()\n\n        self.action_dim = action_dim\n        self.num_bins = num_bins\n        self.max_steps = max_steps\n\n        # Action embedding\n        self.action_embed = nn.Embedding(num_bins * action_dim, hidden_dim)\n\n        # Transformer decoder\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=hidden_dim,\n            nhead=8,\n            dim_feedforward=2048,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=6)\n\n        # Output heads for each action dimension\n        self.action_heads = nn.ModuleList([\n            nn.Linear(hidden_dim, num_bins)\n            for _ in range(action_dim)\n        ])\n\n    def forward(\n        self,\n        context: torch.Tensor,\n        target_actions: torch.Tensor = None\n    ) -> torch.Tensor:\n        """\n        Args:\n            context: (B, C, 512) vision-language features\n            target_actions: (B, T, 7) ground truth actions for training\n        Returns:\n            action_logits: (B, T, 7, num_bins)\n        """\n        B = context.shape[0]\n\n        if target_actions is not None:\n            # Training: teacher forcing\n            T = target_actions.shape[1]\n\n            # Embed actions\n            action_indices = target_actions * self.action_dim + torch.arange(\n                self.action_dim, device=target_actions.device\n            )\n            action_embeddings = self.action_embed(action_indices.flatten(-2))\n\n            # Decode\n            decoded = self.transformer(\n                action_embeddings,\n                context,\n                tgt_mask=self._generate_causal_mask(T)\n            )\n\n            # Predict next action\n            logits = []\n            for i, head in enumerate(self.action_heads):\n                logits.append(head(decoded))\n\n            return torch.stack(logits, dim=-2)\n        else:\n            # Inference: autoregressive generation\n            return self._generate(context)\n\n    def _generate(self, context: torch.Tensor) -> torch.Tensor:\n        """Generate actions autoregressively."""\n        B = context.shape[0]\n        device = context.device\n\n        actions = []\n        action_tokens = torch.zeros(B, 1, self.action_dim, dtype=torch.long, device=device)\n\n        for t in range(self.max_steps):\n            # Embed current actions\n            action_embeddings = self.action_embed(\n                action_tokens * self.action_dim + torch.arange(self.action_dim, device=device)\n            )\n\n            # Decode\n            decoded = self.transformer(action_embeddings, context)\n\n            # Predict next action\n            next_action = []\n            for i, head in enumerate(self.action_heads):\n                logits = head(decoded[:, -1])\n                token = torch.argmax(logits, dim=-1)\n                next_action.append(token)\n\n            next_action = torch.stack(next_action, dim=-1).unsqueeze(1)\n            action_tokens = torch.cat([action_tokens, next_action], dim=1)\n            actions.append(next_action)\n\n        return torch.cat(actions, dim=1)\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"55-complete-vla-model",children:"5.5 Complete VLA Model"}),"\n",(0,t.jsx)(e.h3,{id:"openvla-architecture",children:"OpenVLA Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class OpenVLA(nn.Module):\n    """Open-source Vision-Language-Action model."""\n\n    def __init__(self, config):\n        super().__init__()\n\n        # Vision encoder\n        self.vision_encoder = MultiViewEncoder(\n            num_views=config.num_cameras\n        )\n\n        # Language encoder\n        self.language_encoder = InstructionEncoder(\n            model_name=config.language_model\n        )\n\n        # Vision-language grounding\n        self.grounding = VisionLanguageGrounding(\n            hidden_dim=config.hidden_dim\n        )\n\n        # Action decoder\n        self.action_decoder = ActionDecoder(\n            hidden_dim=config.hidden_dim,\n            action_dim=config.action_dim,\n            num_bins=config.num_bins\n        )\n\n        # Action tokenizer\n        self.tokenizer = ActionTokenizer(\n            num_bins=config.num_bins,\n            action_dim=config.action_dim\n        )\n\n    def forward(\n        self,\n        images: torch.Tensor,\n        instructions: list[str],\n        actions: torch.Tensor = None\n    ):\n        """\n        Args:\n            images: (B, num_views, 3, 224, 224)\n            instructions: List of instruction strings\n            actions: (B, T, action_dim) target actions\n        """\n        # Encode vision\n        vision_features = self.vision_encoder(images)\n\n        # Encode language\n        language_features = self.language_encoder(instructions)\n\n        # Ground language to vision\n        grounded_features = self.grounding(vision_features, language_features)\n\n        # Tokenize actions for training\n        if actions is not None:\n            action_tokens = self.tokenizer.encode(actions)\n        else:\n            action_tokens = None\n\n        # Decode actions\n        action_logits = self.action_decoder(grounded_features, action_tokens)\n\n        return action_logits\n\n    def predict(\n        self,\n        images: torch.Tensor,\n        instruction: str\n    ) -> torch.Tensor:\n        """Predict actions for a single instruction."""\n        with torch.no_grad():\n            logits = self.forward(images.unsqueeze(0), [instruction])\n            tokens = torch.argmax(logits, dim=-1)\n            actions = self.tokenizer.decode(tokens)\n        return actions.squeeze(0)\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"56-training-vla-models",children:"5.6 Training VLA Models"}),"\n",(0,t.jsx)(e.h3,{id:"data-collection",children:"Data Collection"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class RobotDemonstrationDataset(torch.utils.data.Dataset):\n    """Dataset of robot demonstrations."""\n\n    def __init__(self, data_path: str, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n\n        # Load demonstration index\n        self.demos = self._load_demos()\n\n    def _load_demos(self):\n        """Load demonstration metadata."""\n        demos = []\n        for demo_dir in Path(self.data_path).iterdir():\n            if demo_dir.is_dir():\n                metadata = json.load(open(demo_dir / "metadata.json"))\n                demos.append({\n                    "path": demo_dir,\n                    "instruction": metadata["instruction"],\n                    "num_steps": metadata["num_steps"]\n                })\n        return demos\n\n    def __getitem__(self, idx):\n        demo = self.demos[idx]\n\n        # Load images\n        images = []\n        for cam in ["front", "left", "right"]:\n            img_path = demo["path"] / f"{cam}_0.png"\n            img = Image.open(img_path)\n            if self.transform:\n                img = self.transform(img)\n            images.append(img)\n        images = torch.stack(images)\n\n        # Load actions\n        actions = np.load(demo["path"] / "actions.npy")\n        actions = torch.from_numpy(actions).float()\n\n        return {\n            "images": images,\n            "instruction": demo["instruction"],\n            "actions": actions\n        }\n'})}),"\n",(0,t.jsx)(e.h3,{id:"training-loop",children:"Training Loop"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class VLATrainer:\n    """Train VLA model on demonstration data."""\n\n    def __init__(self, model, dataset, config):\n        self.model = model\n        self.dataset = dataset\n        self.config = config\n\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config.learning_rate,\n            weight_decay=config.weight_decay\n        )\n\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=config.num_epochs\n        )\n\n    def train_epoch(self, dataloader):\n        self.model.train()\n        total_loss = 0\n\n        for batch in tqdm(dataloader):\n            images = batch["images"].to(self.config.device)\n            instructions = batch["instruction"]\n            actions = batch["actions"].to(self.config.device)\n\n            # Forward pass\n            action_logits = self.model(images, instructions, actions)\n\n            # Compute loss (cross-entropy over action tokens)\n            action_tokens = self.model.tokenizer.encode(actions)\n            loss = F.cross_entropy(\n                action_logits.reshape(-1, self.config.num_bins),\n                action_tokens.reshape(-1)\n            )\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n\n            total_loss += loss.item()\n\n        self.scheduler.step()\n        return total_loss / len(dataloader)\n\n    def evaluate(self, dataloader):\n        """Evaluate action prediction accuracy."""\n        self.model.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for batch in dataloader:\n                images = batch["images"].to(self.config.device)\n                instructions = batch["instruction"]\n                actions = batch["actions"].to(self.config.device)\n\n                # Predict\n                action_logits = self.model(images, instructions)\n                predicted = torch.argmax(action_logits, dim=-1)\n\n                # Compare\n                target_tokens = self.model.tokenizer.encode(actions)\n                correct += (predicted == target_tokens).sum().item()\n                total += target_tokens.numel()\n\n        return correct / total\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"57-deployment-on-humanoids",children:"5.7 Deployment on Humanoids"}),"\n",(0,t.jsx)(e.h3,{id:"ros-2-vla-node",children:"ROS 2 VLA Node"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport torch\n\nclass VLAControlNode(Node):\n    """ROS 2 node for VLA-based humanoid control."""\n\n    def __init__(self):\n        super().__init__(\'vla_control\')\n\n        # Load model\n        self.model = OpenVLA.load_pretrained("openvla-7b")\n        self.model.eval()\n        self.model.to("cuda")\n\n        self.bridge = CvBridge()\n        self.current_images = {}\n\n        # Subscribers\n        self.image_subs = {\n            "front": self.create_subscription(\n                Image, \'/camera/front/image\',\n                lambda msg: self.image_callback(msg, "front"), 10\n            ),\n            "left": self.create_subscription(\n                Image, \'/camera/left/image\',\n                lambda msg: self.image_callback(msg, "left"), 10\n            ),\n        }\n\n        self.instruction_sub = self.create_subscription(\n            String, \'/instruction\', self.instruction_callback, 10\n        )\n\n        # Publisher\n        self.action_pub = self.create_publisher(\n            JointState, \'/joint_commands\', 10\n        )\n\n        self.current_instruction = None\n\n        # Control loop\n        self.timer = self.create_timer(0.1, self.control_loop)  # 10 Hz\n\n    def image_callback(self, msg, camera_name):\n        """Store latest image from each camera."""\n        cv_image = self.bridge.imgmsg_to_cv2(msg, "rgb8")\n        self.current_images[camera_name] = cv_image\n\n    def instruction_callback(self, msg):\n        """Receive new instruction."""\n        self.current_instruction = msg.data\n        self.get_logger().info(f"New instruction: {msg.data}")\n\n    def control_loop(self):\n        """Execute VLA model and publish actions."""\n        if self.current_instruction is None:\n            return\n\n        if len(self.current_images) < 2:\n            return\n\n        # Prepare images\n        images = []\n        for cam in ["front", "left"]:\n            img = self.current_images.get(cam)\n            if img is not None:\n                img = self.preprocess(img)\n                images.append(img)\n\n        images = torch.stack(images).unsqueeze(0).to("cuda")\n\n        # Predict action\n        with torch.no_grad():\n            action = self.model.predict(images, self.current_instruction)\n\n        # Convert to joint command\n        joint_msg = self.action_to_joint_state(action)\n        self.action_pub.publish(joint_msg)\n\n    def action_to_joint_state(self, action):\n        """Convert VLA action to JointState message."""\n        msg = JointState()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.name = [\n            "shoulder_pan", "shoulder_lift", "elbow",\n            "wrist_1", "wrist_2", "wrist_3", "gripper"\n        ]\n        msg.position = action.cpu().numpy().tolist()\n        return msg\n\ndef main():\n    rclpy.init()\n    node = VLAControlNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"58-advanced-topics",children:"5.8 Advanced Topics"}),"\n",(0,t.jsx)(e.h3,{id:"chain-of-thought-reasoning",children:"Chain-of-Thought Reasoning"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class VLAWithReasoning(OpenVLA):\n    """VLA with intermediate reasoning steps."""\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        # Reasoning head\n        self.reasoning_head = nn.Linear(config.hidden_dim, config.vocab_size)\n\n    def forward_with_reasoning(self, images, instruction):\n        """Generate reasoning chain before action."""\n        # Encode inputs\n        vision_features = self.vision_encoder(images)\n        language_features = self.language_encoder([instruction])\n        grounded = self.grounding(vision_features, language_features)\n\n        # Generate reasoning tokens\n        reasoning_tokens = self._generate_reasoning(grounded)\n\n        # Decode reasoning to text\n        reasoning_text = self.tokenizer.decode(reasoning_tokens)\n\n        # Append reasoning to context\n        enhanced_features = self._incorporate_reasoning(\n            grounded, reasoning_tokens\n        )\n\n        # Generate actions\n        actions = self.action_decoder(enhanced_features)\n\n        return actions, reasoning_text\n'})}),"\n",(0,t.jsx)(e.h3,{id:"multi-task-learning",children:"Multi-Task Learning"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class MultiTaskVLA(OpenVLA):\n    """VLA for multiple manipulation tasks."""\n\n    TASKS = [\n        "pick_and_place",\n        "pour",\n        "open_drawer",\n        "press_button",\n        "wipe_surface"\n    ]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        # Task-specific heads\n        self.task_heads = nn.ModuleDict({\n            task: ActionDecoder(\n                hidden_dim=config.hidden_dim,\n                action_dim=config.action_dim\n            )\n            for task in self.TASKS\n        })\n\n        # Task classifier\n        self.task_classifier = nn.Linear(config.hidden_dim, len(self.TASKS))\n\n    def forward(self, images, instruction, task=None):\n        # Encode\n        grounded = self._encode(images, instruction)\n\n        # Classify task if not provided\n        if task is None:\n            task_logits = self.task_classifier(grounded.mean(dim=1))\n            task_idx = torch.argmax(task_logits, dim=-1)\n            task = self.TASKS[task_idx]\n\n        # Use task-specific decoder\n        actions = self.task_heads[task](grounded)\n\n        return actions, task\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"VLA models combine vision, language, and action for robot control"}),"\n",(0,t.jsx)(e.li,{children:"Vision encoders extract spatial features from camera images"}),"\n",(0,t.jsx)(e.li,{children:"Language encoders understand natural language instructions"}),"\n",(0,t.jsx)(e.li,{children:"Action decoders generate discretized robot commands"}),"\n",(0,t.jsx)(e.li,{children:"Training requires diverse demonstration datasets"}),"\n",(0,t.jsx)(e.li,{children:"Deployment uses ROS 2 for real-time control"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"What are the three modalities in VLA models?"}),"\n",(0,t.jsx)(e.li,{children:"How does action tokenization work?"}),"\n",(0,t.jsx)(e.li,{children:"What is vision-language grounding?"}),"\n",(0,t.jsx)(e.li,{children:"How are VLA models trained on demonstration data?"}),"\n",(0,t.jsx)(e.li,{children:"What are the challenges in deploying VLA on real robots?"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Exercise 5.1"}),": Implement a simple vision encoder using ResNet-18."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Exercise 5.2"}),": Create an action tokenizer for a 7-DOF arm."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Exercise 5.3"}),": Train a small VLA model on a pick-and-place dataset."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Exercise 5.4"}),": Deploy your VLA model in a ROS 2 simulation."]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"lab-building-a-mini-vla",children:"Lab: Building a Mini-VLA"}),"\n",(0,t.jsx)(e.p,{children:"Complete implementation project:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Create vision encoder with ViT-Small"}),"\n",(0,t.jsx)(e.li,{children:"Implement instruction encoder with DistilBERT"}),"\n",(0,t.jsx)(e.li,{children:"Build action decoder with 4-layer transformer"}),"\n",(0,t.jsx)(e.li,{children:"Train on CALVIN benchmark dataset"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate on held-out manipulation tasks"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453(n,e,i){i.d(e,{R:()=>a,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);