"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[238],{8453(n,e,i){i.d(e,{R:()=>t,x:()=>o});var s=i(6540);const a={},r=s.createContext(a);function t(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),s.createElement(r.Provider,{value:e},n.children)}},9042(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"nvidia-isaac","title":"Chapter 4: NVIDIA Isaac for Humanoid Development","description":"Learning Objectives","source":"@site/docs/04-nvidia-isaac.md","sourceDirName":".","slug":"/nvidia-isaac","permalink":"/nvidia-isaac","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 3: Gazebo Simulation for Humanoid Robots","permalink":"/gazebo-simulation"},"next":{"title":"Chapter 5: Vision-Language-Action Models for Humanoid Control","permalink":"/vla"}}');var a=i(4848),r=i(8453);const t={},o="Chapter 4: NVIDIA Isaac for Humanoid Development",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"4.1 Introduction to NVIDIA Isaac",id:"41-introduction-to-nvidia-isaac",level:2},{value:"What is NVIDIA Isaac?",id:"what-is-nvidia-isaac",level:3},{value:"Why Isaac for Humanoids?",id:"why-isaac-for-humanoids",level:3},{value:"4.2 Isaac Sim Setup",id:"42-isaac-sim-setup",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Installation",id:"installation",level:3},{value:"Python Environment",id:"python-environment",level:3},{value:"4.3 Humanoid Assets in USD",id:"43-humanoid-assets-in-usd",level:2},{value:"USD Format Benefits",id:"usd-format-benefits",level:3},{value:"Converting URDF to USD",id:"converting-urdf-to-usd",level:3},{value:"Humanoid USD Structure",id:"humanoid-usd-structure",level:3},{value:"4.4 Isaac Gym for RL Training",id:"44-isaac-gym-for-rl-training",level:2},{value:"Parallel Environment Setup",id:"parallel-environment-setup",level:3},{value:"Observation and Action Spaces",id:"observation-and-action-spaces",level:3},{value:"PPO Training Loop",id:"ppo-training-loop",level:3},{value:"4.5 Domain Randomization",id:"45-domain-randomization",level:2},{value:"Physics Randomization",id:"physics-randomization",level:3},{value:"Visual Randomization",id:"visual-randomization",level:3},{value:"4.6 Sim-to-Real Transfer",id:"46-sim-to-real-transfer",level:2},{value:"Teacher-Student Training",id:"teacher-student-training",level:3},{value:"Policy Export",id:"policy-export",level:3},{value:"4.7 Isaac ROS Integration",id:"47-isaac-ros-integration",level:2},{value:"Isaac ROS Packages",id:"isaac-ros-packages",level:3},{value:"Visual SLAM with Isaac ROS",id:"visual-slam-with-isaac-ros",level:3},{value:"4.8 Performance Optimization",id:"48-performance-optimization",level:2},{value:"Multi-GPU Training",id:"multi-gpu-training",level:3},{value:"Memory Optimization",id:"memory-optimization",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Lab: End-to-End RL Training Pipeline",id:"lab-end-to-end-rl-training-pipeline",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-4-nvidia-isaac-for-humanoid-development",children:"Chapter 4: NVIDIA Isaac for Humanoid Development"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Set up NVIDIA Isaac Sim for humanoid robot simulation"}),"\n",(0,a.jsx)(e.li,{children:"Use Isaac Gym for reinforcement learning training"}),"\n",(0,a.jsx)(e.li,{children:"Implement sim-to-real transfer techniques"}),"\n",(0,a.jsx)(e.li,{children:"Leverage GPU-accelerated physics for parallel training"}),"\n",(0,a.jsx)(e.li,{children:"Deploy trained policies to real hardware"}),"\n"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"41-introduction-to-nvidia-isaac",children:"4.1 Introduction to NVIDIA Isaac"}),"\n",(0,a.jsx)(e.h3,{id:"what-is-nvidia-isaac",children:"What is NVIDIA Isaac?"}),"\n",(0,a.jsx)(e.p,{children:"NVIDIA Isaac is a comprehensive robotics platform that includes:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac Sim"}),": High-fidelity simulation built on Omniverse"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac Gym"}),": GPU-accelerated RL training environment"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac ROS"}),": Optimized ROS 2 packages for NVIDIA hardware"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac SDK"}),": Tools for robot development and deployment"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"why-isaac-for-humanoids",children:"Why Isaac for Humanoids?"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Feature"}),(0,a.jsx)(e.th,{children:"Benefit for Humanoids"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"PhysX GPU"}),(0,a.jsx)(e.td,{children:"Parallel physics for thousands of robots"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"RTX rendering"}),(0,a.jsx)(e.td,{children:"Photorealistic vision training"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"USD format"}),(0,a.jsx)(e.td,{children:"Industry-standard asset pipeline"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"RL tooling"}),(0,a.jsx)(e.td,{children:"End-to-end policy training"})]})]})]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"42-isaac-sim-setup",children:"4.2 Isaac Sim Setup"}),"\n",(0,a.jsx)(e.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Minimum Requirements:\n- GPU: NVIDIA RTX 2070 or higher\n- CPU: Intel Core i7 or AMD Ryzen 7\n- RAM: 32 GB\n- Storage: 50 GB SSD\n- OS: Ubuntu 20.04/22.04\n\nRecommended:\n- GPU: NVIDIA RTX 4090 or A6000\n- RAM: 64 GB\n- Storage: 100 GB NVMe SSD\n"})}),"\n",(0,a.jsx)(e.h3,{id:"installation",children:"Installation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Install Omniverse Launcher\nwget https://install.launcher.omniverse.nvidia.com/installers/\\\nomniverse-launcher-linux.AppImage\n\nchmod +x omniverse-launcher-linux.AppImage\n./omniverse-launcher-linux.AppImage\n\n# Install Isaac Sim from Omniverse Launcher\n# Navigate to Exchange -> Isaac Sim -> Install\n\n# Verify installation\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/isaac-sim.sh --help\n"})}),"\n",(0,a.jsx)(e.h3,{id:"python-environment",children:"Python Environment"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Create conda environment\nconda create -n isaac python=3.10\nconda activate isaac\n\n# Install Isaac Sim Python packages\npip install isaacsim-rl isaacsim-robot isaacsim-sensor\n\n# Or use the bundled Python\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/python.sh -m pip install torch\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"43-humanoid-assets-in-usd",children:"4.3 Humanoid Assets in USD"}),"\n",(0,a.jsx)(e.h3,{id:"usd-format-benefits",children:"USD Format Benefits"}),"\n",(0,a.jsx)(e.p,{children:"Universal Scene Description (USD) provides:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Hierarchical scene composition"}),"\n",(0,a.jsx)(e.li,{children:"Non-destructive editing"}),"\n",(0,a.jsx)(e.li,{children:"Collaboration workflows"}),"\n",(0,a.jsx)(e.li,{children:"Industry standard (Pixar)"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"converting-urdf-to-usd",children:"Converting URDF to USD"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from omni.isaac.urdf import _urdf\nfrom pxr import Usd, UsdPhysics\n\ndef convert_humanoid_urdf(urdf_path: str, usd_path: str):\n    """Convert URDF to USD with physics properties."""\n\n    # Import URDF\n    import_config = _urdf.ImportConfig()\n    import_config.merge_fixed_joints = False\n    import_config.fix_base = False\n    import_config.make_default_prim = True\n    import_config.create_physics_scene = True\n\n    # Set physics properties\n    import_config.default_drive_type = _urdf.UrdfJointTargetType.JOINT_DRIVE_POSITION\n    import_config.default_drive_strength = 1000.0\n    import_config.default_position_drive_damping = 100.0\n\n    # Convert\n    result = _urdf.import_robot(\n        urdf_path,\n        usd_path,\n        import_config\n    )\n\n    return result\n'})}),"\n",(0,a.jsx)(e.h3,{id:"humanoid-usd-structure",children:"Humanoid USD Structure"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"humanoid.usd\n\u251c\u2500\u2500 /World\n\u2502   \u251c\u2500\u2500 /PhysicsScene\n\u2502   \u251c\u2500\u2500 /GroundPlane\n\u2502   \u2514\u2500\u2500 /Humanoid\n\u2502       \u251c\u2500\u2500 /torso (root link)\n\u2502       \u2502   \u251c\u2500\u2500 /left_hip_joint (revolute)\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 /left_thigh\n\u2502       \u2502   \u2502       \u2514\u2500\u2500 /left_knee_joint\n\u2502       \u2502   \u2502           \u2514\u2500\u2500 /left_shin\n\u2502       \u2502   \u2514\u2500\u2500 /right_hip_joint\n\u2502       \u2502       \u2514\u2500\u2500 /right_thigh\n\u2502       \u2514\u2500\u2500 /Sensors\n\u2502           \u251c\u2500\u2500 /imu_sensor\n\u2502           \u2514\u2500\u2500 /camera_sensor\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"44-isaac-gym-for-rl-training",children:"4.4 Isaac Gym for RL Training"}),"\n",(0,a.jsx)(e.h3,{id:"parallel-environment-setup",children:"Parallel Environment Setup"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import isaacgym\nfrom isaacgym import gymapi, gymtorch\nimport torch\n\nclass HumanoidEnv:\n    """Parallel humanoid training environment."""\n\n    def __init__(self, num_envs: int = 4096, device: str = "cuda"):\n        self.num_envs = num_envs\n        self.device = device\n\n        # Initialize gym\n        self.gym = gymapi.acquire_gym()\n\n        # Simulation parameters\n        sim_params = gymapi.SimParams()\n        sim_params.dt = 1.0 / 60.0\n        sim_params.substeps = 2\n        sim_params.up_axis = gymapi.UP_AXIS_Z\n        sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.81)\n\n        # PhysX parameters\n        sim_params.physx.solver_type = 1\n        sim_params.physx.num_position_iterations = 4\n        sim_params.physx.num_velocity_iterations = 1\n        sim_params.physx.contact_offset = 0.01\n        sim_params.physx.rest_offset = 0.0\n\n        # Create sim\n        self.sim = self.gym.create_sim(\n            0, 0,  # GPU device IDs\n            gymapi.SIM_PHYSX,\n            sim_params\n        )\n\n        self._create_envs()\n\n    def _create_envs(self):\n        """Create parallel environments."""\n        # Load humanoid asset\n        asset_options = gymapi.AssetOptions()\n        asset_options.fix_base_link = False\n        asset_options.angular_damping = 0.01\n\n        humanoid_asset = self.gym.load_asset(\n            self.sim,\n            "assets",\n            "humanoid.urdf",\n            asset_options\n        )\n\n        # Environment spacing\n        env_spacing = 2.0\n        env_lower = gymapi.Vec3(-env_spacing, -env_spacing, 0.0)\n        env_upper = gymapi.Vec3(env_spacing, env_spacing, env_spacing)\n\n        self.envs = []\n        self.actors = []\n\n        for i in range(self.num_envs):\n            env = self.gym.create_env(self.sim, env_lower, env_upper, int(self.num_envs ** 0.5))\n\n            pose = gymapi.Transform()\n            pose.p = gymapi.Vec3(0.0, 0.0, 1.0)\n\n            actor = self.gym.create_actor(env, humanoid_asset, pose, f"humanoid_{i}", i, 0)\n\n            self.envs.append(env)\n            self.actors.append(actor)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"observation-and-action-spaces",children:"Observation and Action Spaces"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class HumanoidEnv:\n    # ... continued\n\n    def _get_observations(self) -> torch.Tensor:\n        """Get observation tensor for all environments."""\n        # Root state: position (3) + orientation (4) + linear vel (3) + angular vel (3)\n        root_states = self.root_tensor[:, :13]\n\n        # Joint states: positions (num_dof) + velocities (num_dof)\n        dof_pos = self.dof_pos_tensor\n        dof_vel = self.dof_vel_tensor\n\n        # Concatenate observations\n        obs = torch.cat([\n            root_states[:, 2:3],    # Height\n            root_states[:, 3:7],    # Orientation (quaternion)\n            root_states[:, 7:10],   # Linear velocity\n            root_states[:, 10:13],  # Angular velocity\n            dof_pos,                 # Joint positions\n            dof_vel,                 # Joint velocities\n        ], dim=-1)\n\n        return obs\n\n    def _compute_reward(self) -> torch.Tensor:\n        """Compute reward for walking task."""\n        # Forward velocity reward\n        forward_vel = self.root_tensor[:, 7]  # x velocity\n        vel_reward = torch.exp(-torch.abs(forward_vel - 1.0))\n\n        # Upright reward\n        up_vec = self._get_up_vector()\n        upright_reward = torch.sum(up_vec * torch.tensor([0, 0, 1], device=self.device), dim=-1)\n\n        # Energy penalty\n        energy_penalty = torch.sum(torch.abs(self.torques) * torch.abs(self.dof_vel_tensor), dim=-1)\n\n        # Total reward\n        reward = vel_reward + 0.5 * upright_reward - 0.01 * energy_penalty\n\n        return reward\n\n    def step(self, actions: torch.Tensor):\n        """Step all environments."""\n        # Apply actions as joint torques\n        self.gym.set_dof_actuation_force_tensor(\n            self.sim,\n            gymtorch.unwrap_tensor(actions)\n        )\n\n        # Simulate\n        self.gym.simulate(self.sim)\n        self.gym.fetch_results(self.sim, True)\n\n        # Refresh tensors\n        self.gym.refresh_actor_root_state_tensor(self.sim)\n        self.gym.refresh_dof_state_tensor(self.sim)\n\n        # Get new observations and rewards\n        obs = self._get_observations()\n        rewards = self._compute_reward()\n        dones = self._check_termination()\n\n        return obs, rewards, dones, {}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"ppo-training-loop",children:"PPO Training Loop"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from rl_games.algos_torch import ppo\n\nclass HumanoidTrainer:\n    """PPO trainer for humanoid locomotion."""\n\n    def __init__(self, env, config):\n        self.env = env\n        self.config = config\n\n        # Policy network\n        self.policy = ActorCritic(\n            obs_dim=env.obs_dim,\n            action_dim=env.action_dim,\n            hidden_dims=[256, 256, 128]\n        ).to(env.device)\n\n        self.optimizer = torch.optim.Adam(\n            self.policy.parameters(),\n            lr=3e-4\n        )\n\n    def train(self, num_iterations: int = 10000):\n        """Train the policy."""\n        for iteration in range(num_iterations):\n            # Collect rollouts\n            with torch.no_grad():\n                obs = self.env.reset()\n\n                for step in range(self.config.horizon):\n                    actions, log_probs, values = self.policy(obs)\n                    obs, rewards, dones, _ = self.env.step(actions)\n\n                    # Store transition\n                    self.buffer.add(obs, actions, rewards, dones, log_probs, values)\n\n            # PPO update\n            for epoch in range(self.config.num_epochs):\n                for batch in self.buffer.get_batches():\n                    loss = self._compute_ppo_loss(batch)\n\n                    self.optimizer.zero_grad()\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n                    self.optimizer.step()\n\n            # Logging\n            if iteration % 100 == 0:\n                print(f"Iteration {iteration}, Reward: {rewards.mean():.2f}")\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"45-domain-randomization",children:"4.5 Domain Randomization"}),"\n",(0,a.jsx)(e.h3,{id:"physics-randomization",children:"Physics Randomization"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class DomainRandomization:\n    """Domain randomization for sim-to-real transfer."""\n\n    def __init__(self, env):\n        self.env = env\n\n        # Randomization ranges\n        self.mass_range = (0.8, 1.2)  # Scale factor\n        self.friction_range = (0.5, 1.5)\n        self.motor_strength_range = (0.9, 1.1)\n        self.gravity_range = (-10.0, -9.5)\n\n    def randomize(self):\n        """Apply randomization to all environments."""\n        for i, env in enumerate(self.env.envs):\n            # Randomize mass\n            mass_scale = np.random.uniform(*self.mass_range)\n            self._scale_link_masses(env, mass_scale)\n\n            # Randomize friction\n            friction = np.random.uniform(*self.friction_range)\n            self._set_friction(env, friction)\n\n            # Randomize motor strength\n            motor_scale = np.random.uniform(*self.motor_strength_range)\n            self._scale_motor_strength(env, motor_scale)\n\n    def _scale_link_masses(self, env, scale):\n        """Scale masses of all links."""\n        props = self.env.gym.get_actor_rigid_body_properties(env, 0)\n        for prop in props:\n            prop.mass *= scale\n        self.env.gym.set_actor_rigid_body_properties(env, 0, props)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"visual-randomization",children:"Visual Randomization"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'def randomize_visual(env, camera):\n    """Randomize visual appearance for vision training."""\n    # Lighting\n    light_intensity = np.random.uniform(0.5, 2.0)\n    light_color = np.random.uniform(0.8, 1.0, size=3)\n\n    # Textures\n    texture_options = ["wood", "metal", "concrete", "tile"]\n    floor_texture = np.random.choice(texture_options)\n\n    # Camera noise\n    camera_noise_std = np.random.uniform(0.01, 0.05)\n\n    return {\n        "light_intensity": light_intensity,\n        "light_color": light_color,\n        "floor_texture": floor_texture,\n        "camera_noise": camera_noise_std\n    }\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"46-sim-to-real-transfer",children:"4.6 Sim-to-Real Transfer"}),"\n",(0,a.jsx)(e.h3,{id:"teacher-student-training",children:"Teacher-Student Training"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class TeacherStudent:\n    """Teacher-student framework for sim-to-real."""\n\n    def __init__(self):\n        # Teacher has access to privileged information\n        self.teacher = ActorCritic(\n            obs_dim=PRIVILEGED_OBS_DIM,  # Includes ground truth\n            action_dim=ACTION_DIM\n        )\n\n        # Student only sees real-world observations\n        self.student = ActorCritic(\n            obs_dim=REAL_OBS_DIM,  # Only sensors\n            action_dim=ACTION_DIM\n        )\n\n    def train_teacher(self, env):\n        """Train teacher with privileged observations."""\n        # Teacher sees: true contact states, terrain info, etc.\n        pass\n\n    def distill_to_student(self, env):\n        """Distill teacher knowledge to student."""\n        for batch in self.dataset:\n            # Student tries to match teacher actions\n            teacher_actions = self.teacher(batch.privileged_obs)\n            student_actions = self.student(batch.real_obs)\n\n            loss = F.mse_loss(student_actions, teacher_actions.detach())\n\n            self.student_optimizer.zero_grad()\n            loss.backward()\n            self.student_optimizer.step()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"policy-export",children:"Policy Export"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'def export_policy_for_deployment(policy, path: str):\n    """Export trained policy for real robot deployment."""\n    # Trace the model\n    example_obs = torch.zeros(1, OBS_DIM)\n    traced_model = torch.jit.trace(policy.actor, example_obs)\n\n    # Save\n    traced_model.save(path)\n\n    print(f"Policy exported to {path}")\n\ndef create_ros2_node(policy_path: str):\n    """Generate ROS 2 node for policy deployment."""\n    template = \'\'\'\nimport rclpy\nfrom rclpy.node import Node\nimport torch\n\nclass PolicyNode(Node):\n    def __init__(self):\n        super().__init__(\'policy_node\')\n        self.policy = torch.jit.load("{policy_path}")\n        self.policy.eval()\n\n        self.obs_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.obs_callback, 10\n        )\n        self.action_pub = self.create_publisher(\n            JointTrajectory, \'/joint_trajectory\', 10\n        )\n\n    def obs_callback(self, msg):\n        obs = self._msg_to_tensor(msg)\n        with torch.no_grad():\n            action = self.policy(obs)\n        self._publish_action(action)\n\'\'\'\n    return template.format(policy_path=policy_path)\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"47-isaac-ros-integration",children:"4.7 Isaac ROS Integration"}),"\n",(0,a.jsx)(e.h3,{id:"isaac-ros-packages",children:"Isaac ROS Packages"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Install Isaac ROS\nmkdir -p ~/workspaces/isaac_ros-dev/src\ncd ~/workspaces/isaac_ros-dev/src\n\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_nvblox.git\n\n# Build with Docker\ncd ~/workspaces/isaac_ros-dev/src/isaac_ros_common\n./scripts/run_dev.sh\ncolcon build --symlink-install\n"})}),"\n",(0,a.jsx)(e.h3,{id:"visual-slam-with-isaac-ros",children:"Visual SLAM with Isaac ROS"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    \"\"\"Launch visual SLAM for humanoid navigation.\"\"\"\n\n    visual_slam = Node(\n        package='isaac_ros_visual_slam',\n        executable='isaac_ros_visual_slam',\n        parameters=[{\n            'denoise_input_images': True,\n            'rectified_images': True,\n            'enable_imu_fusion': True,\n            'gyro_noise_density': 0.00016,\n            'accelerometer_noise_density': 0.00024,\n        }],\n        remappings=[\n            ('camera/image_raw', '/humanoid/camera/image'),\n            ('camera/camera_info', '/humanoid/camera/info'),\n            ('imu', '/humanoid/imu'),\n        ]\n    )\n\n    return LaunchDescription([visual_slam])\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"48-performance-optimization",children:"4.8 Performance Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"multi-gpu-training",children:"Multi-GPU Training"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class MultiGPUTrainer:\n    """Distributed training across multiple GPUs."""\n\n    def __init__(self, num_gpus: int = 4):\n        self.num_gpus = num_gpus\n\n        # Create environments on each GPU\n        self.envs = []\n        for gpu_id in range(num_gpus):\n            env = HumanoidEnv(\n                num_envs=4096,\n                device=f"cuda:{gpu_id}"\n            )\n            self.envs.append(env)\n\n        # Total environments\n        self.total_envs = 4096 * num_gpus  # 16,384 parallel robots\n\n    def collect_experience(self):\n        """Collect experience from all GPUs."""\n        all_obs = []\n        all_rewards = []\n\n        for env in self.envs:\n            obs, rewards, _, _ = env.step(self.policy(env.obs))\n            all_obs.append(obs)\n            all_rewards.append(rewards)\n\n        return torch.cat(all_obs), torch.cat(all_rewards)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"memory-optimization",children:"Memory Optimization"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Technique"}),(0,a.jsx)(e.th,{children:"Memory Saving"}),(0,a.jsx)(e.th,{children:"Implementation"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Mixed precision"}),(0,a.jsx)(e.td,{children:"50%"}),(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"torch.cuda.amp"})})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Gradient checkpointing"}),(0,a.jsx)(e.td,{children:"30%"}),(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"torch.utils.checkpoint"})})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Shared memory tensors"}),(0,a.jsx)(e.td,{children:"20%"}),(0,a.jsx)(e.td,{children:(0,a.jsx)(e.code,{children:"gymtorch.wrap_tensor"})})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:"Batch size tuning"}),(0,a.jsx)(e.td,{children:"Variable"}),(0,a.jsx)(e.td,{children:"Dynamic batching"})]})]})]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"NVIDIA Isaac provides GPU-accelerated simulation for humanoids"}),"\n",(0,a.jsx)(e.li,{children:"Isaac Gym enables massively parallel RL training"}),"\n",(0,a.jsx)(e.li,{children:"Domain randomization improves sim-to-real transfer"}),"\n",(0,a.jsx)(e.li,{children:"Teacher-student training bridges the reality gap"}),"\n",(0,a.jsx)(e.li,{children:"Isaac ROS integrates with real robot deployment"}),"\n"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"What are the main components of the NVIDIA Isaac platform?"}),"\n",(0,a.jsx)(e.li,{children:"How does Isaac Gym achieve parallel environment simulation?"}),"\n",(0,a.jsx)(e.li,{children:"What is domain randomization and why is it important?"}),"\n",(0,a.jsx)(e.li,{children:"Explain the teacher-student framework for sim-to-real transfer."}),"\n",(0,a.jsx)(e.li,{children:"How do you export a trained policy for real robot deployment?"}),"\n"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Exercise 4.1"}),": Set up Isaac Sim and load a humanoid model from URDF."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Exercise 4.2"}),": Implement a parallel environment with 1024 humanoids in Isaac Gym."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Exercise 4.3"}),": Train a walking policy using PPO with domain randomization."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Exercise 4.4"}),": Export your trained policy and create a ROS 2 node for deployment."]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"lab-end-to-end-rl-training-pipeline",children:"Lab: End-to-End RL Training Pipeline"}),"\n",(0,a.jsx)(e.p,{children:"Build a complete training pipeline:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Load humanoid asset in Isaac Gym"}),"\n",(0,a.jsx)(e.li,{children:"Define observation/action spaces for walking"}),"\n",(0,a.jsx)(e.li,{children:"Implement reward function for forward locomotion"}),"\n",(0,a.jsx)(e.li,{children:"Apply domain randomization"}),"\n",(0,a.jsx)(e.li,{children:"Train with PPO for 10,000 iterations"}),"\n",(0,a.jsx)(e.li,{children:"Export policy for ROS 2 deployment"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}}}]);