"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[855],{5565(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"capstone","title":"Chapter 6: Capstone Project - Building an Autonomous Humanoid System","description":"Learning Objectives","source":"@site/docs/06-capstone.md","sourceDirName":".","slug":"/capstone","permalink":"/Ai-Driven-Book-with-rag-chatbot/capstone","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 5: Vision-Language-Action Models for Humanoid Control","permalink":"/Ai-Driven-Book-with-rag-chatbot/vla"}}');var s=t(4848),a=t(8453);const i={},r="Chapter 6: Capstone Project - Building an Autonomous Humanoid System",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"6.1 Project Overview",id:"61-project-overview",level:2},{value:"Capstone Challenge",id:"capstone-challenge",level:3},{value:"System Requirements",id:"system-requirements",level:3},{value:"6.2 System Architecture",id:"62-system-architecture",level:2},{value:"ROS 2 Node Graph",id:"ros-2-node-graph",level:3},{value:"Message Types",id:"message-types",level:3},{value:"6.3 Perception Module",id:"63-perception-module",level:2},{value:"Multi-Modal Perception",id:"multi-modal-perception",level:3},{value:"Speech Recognition",id:"speech-recognition",level:3},{value:"6.4 Planning Module",id:"64-planning-module",level:2},{value:"Task Planner",id:"task-planner",level:3},{value:"Motion Planning with MoveIt2",id:"motion-planning-with-moveit2",level:3},{value:"6.5 Control Module",id:"65-control-module",level:2},{value:"Whole-Body Control",id:"whole-body-control",level:3},{value:"Locomotion Controller",id:"locomotion-controller",level:3},{value:"6.6 System Integration",id:"66-system-integration",level:2},{value:"State Machine Coordinator",id:"state-machine-coordinator",level:3},{value:"System Launch",id:"system-launch",level:3},{value:"6.7 Testing and Evaluation",id:"67-testing-and-evaluation",level:2},{value:"Unit Tests",id:"unit-tests",level:3},{value:"Integration Tests",id:"integration-tests",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"6.8 Documentation and Presentation",id:"68-documentation-and-presentation",level:2},{value:"Project Documentation",id:"project-documentation",level:3},{value:"Configuration",id:"configuration",level:2},{value:"Operation Manual",id:"operation-manual",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-6-capstone-project---building-an-autonomous-humanoid-system",children:"Chapter 6: Capstone Project - Building an Autonomous Humanoid System"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Integrate all concepts from previous chapters into a complete system"}),"\n",(0,s.jsx)(e.li,{children:"Design and implement a full humanoid robot pipeline"}),"\n",(0,s.jsx)(e.li,{children:"Deploy perception, planning, and control on real hardware"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate system performance and iterate on design"}),"\n",(0,s.jsx)(e.li,{children:"Document and present your humanoid robotics project"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"61-project-overview",children:"6.1 Project Overview"}),"\n",(0,s.jsx)(e.h3,{id:"capstone-challenge",children:"Capstone Challenge"}),"\n",(0,s.jsx)(e.p,{children:"Build an autonomous humanoid robot system that can:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Navigate"})," through an indoor environment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Recognize"})," objects and people"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Manipulate"})," objects based on voice commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Communicate"})," status and ask for clarification"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Autonomous Humanoid System                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                   \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502   Perception  \u2502\u2500\u2500\u2500\u25b6\u2502   Planning    \u2502\u2500\u2500\u2500\u25b6\u2502   Execution   \u2502   \u2502\n\u2502   \u2502               \u2502    \u2502               \u2502    \u2502               \u2502   \u2502\n\u2502   \u2502 \u2022 Vision      \u2502    \u2502 \u2022 Navigation  \u2502    \u2502 \u2022 Locomotion  \u2502   \u2502\n\u2502   \u2502 \u2022 Speech      \u2502    \u2502 \u2022 Task        \u2502    \u2502 \u2022 Manipulation\u2502   \u2502\n\u2502   \u2502 \u2022 SLAM        \u2502    \u2502 \u2022 Motion      \u2502    \u2502 \u2022 Balance     \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502           \u2502                    \u2502                    \u2502             \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502                                \u25bc                                  \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502                    \u2502    Coordination   \u2502                         \u2502\n\u2502                    \u2502   State Machine   \u2502                         \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Component"}),(0,s.jsx)(e.th,{children:"Requirement"}),(0,s.jsx)(e.th,{children:"Technology"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Perception"}),(0,s.jsx)(e.td,{children:"Object detection >90% accuracy"}),(0,s.jsx)(e.td,{children:"YOLOv8 + SAM"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Navigation"}),(0,s.jsx)(e.td,{children:"Autonomous in 10x10m space"}),(0,s.jsx)(e.td,{children:"Nav2 + SLAM"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Manipulation"}),(0,s.jsx)(e.td,{children:"Pick objects within reach"}),(0,s.jsx)(e.td,{children:"MoveIt2 + VLA"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Speech"}),(0,s.jsx)(e.td,{children:"Understand 50+ commands"}),(0,s.jsx)(e.td,{children:"Whisper + LLM"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Response time"}),(0,s.jsx)(e.td,{children:"Less than 500ms for commands"}),(0,s.jsx)(e.td,{children:"ROS 2 real-time"})]})]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"62-system-architecture",children:"6.2 System Architecture"}),"\n",(0,s.jsx)(e.h3,{id:"ros-2-node-graph",children:"ROS 2 Node Graph"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# launch/humanoid_system.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import IncludeLaunchDescription\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Perception Stack\n        Node(\n            package='humanoid_perception',\n            executable='camera_node',\n            name='camera',\n            parameters=[{'device': '/dev/video0'}]\n        ),\n        Node(\n            package='humanoid_perception',\n            executable='object_detector',\n            name='detector',\n        ),\n        Node(\n            package='humanoid_perception',\n            executable='slam_node',\n            name='slam',\n        ),\n\n        # Planning Stack\n        Node(\n            package='humanoid_planning',\n            executable='task_planner',\n            name='task_planner',\n        ),\n        Node(\n            package='nav2_bringup',\n            executable='navigation_launch.py',\n            name='navigation',\n        ),\n\n        # Control Stack\n        Node(\n            package='humanoid_control',\n            executable='locomotion_controller',\n            name='locomotion',\n        ),\n        Node(\n            package='humanoid_control',\n            executable='manipulation_controller',\n            name='manipulation',\n        ),\n\n        # Coordination\n        Node(\n            package='humanoid_coordination',\n            executable='state_machine',\n            name='coordinator',\n        ),\n    ])\n"})}),"\n",(0,s.jsx)(e.h3,{id:"message-types",children:"Message Types"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# msg/HumanoidState.msg\nstd_msgs/Header header\nstring current_task\nstring current_state\ngeometry_msgs/Pose robot_pose\nfloat32[] joint_positions\nbool is_balanced\nbool gripper_closed\n\n# msg/TaskCommand.msg\nstd_msgs/Header header\nstring command_type  # "navigate", "pick", "place", "speak"\nstring target_object\ngeometry_msgs/Pose target_pose\nstring speech_text\n\n# msg/PerceptionResult.msg\nstd_msgs/Header header\nvision_msgs/Detection3DArray detected_objects\nsensor_msgs/PointCloud2 scene_pointcloud\ngeometry_msgs/PoseArray object_poses\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"63-perception-module",children:"6.3 Perception Module"}),"\n",(0,s.jsx)(e.h3,{id:"multi-modal-perception",children:"Multi-Modal Perception"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport torch\nfrom ultralytics import YOLO\nfrom segment_anything import sam_model_registry, SamPredictor\n\nclass PerceptionNode(Node):\n    """Multi-modal perception for humanoid robot."""\n\n    def __init__(self):\n        super().__init__(\'perception\')\n\n        # Load models\n        self.detector = YOLO(\'yolov8x.pt\')\n        self.sam = sam_model_registry["vit_h"](checkpoint="sam_vit_h.pth")\n        self.sam_predictor = SamPredictor(self.sam)\n\n        # Subscribers\n        self.rgb_sub = self.create_subscription(\n            Image, \'/camera/rgb\', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth\', self.depth_callback, 10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(\n            Detection3DArray, \'/detections\', 10\n        )\n        self.segmentation_pub = self.create_publisher(\n            Image, \'/segmentation\', 10\n        )\n\n        self.current_depth = None\n        self.bridge = CvBridge()\n\n    def rgb_callback(self, msg):\n        """Process RGB image for object detection."""\n        image = self.bridge.imgmsg_to_cv2(msg, "rgb8")\n\n        # Detect objects\n        results = self.detector(image)\n\n        # Segment each detection\n        detections = []\n        for box in results[0].boxes:\n            # Get bounding box\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n\n            # SAM segmentation\n            self.sam_predictor.set_image(image)\n            masks, _, _ = self.sam_predictor.predict(\n                box=np.array([x1, y1, x2, y2])\n            )\n\n            # Estimate 3D pose from depth\n            if self.current_depth is not None:\n                pose = self._estimate_pose(masks[0], self.current_depth)\n\n                detection = Detection3D()\n                detection.bbox.center.position.x = pose[0]\n                detection.bbox.center.position.y = pose[1]\n                detection.bbox.center.position.z = pose[2]\n                detection.results[0].hypothesis.class_id = str(int(box.cls))\n                detection.results[0].hypothesis.score = float(box.conf)\n\n                detections.append(detection)\n\n        # Publish\n        msg = Detection3DArray()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.detections = detections\n        self.detection_pub.publish(msg)\n\n    def _estimate_pose(self, mask, depth):\n        """Estimate 3D position from mask and depth."""\n        # Get masked depth values\n        masked_depth = depth * mask\n        valid_depths = masked_depth[masked_depth > 0]\n\n        if len(valid_depths) == 0:\n            return [0, 0, 0]\n\n        # Centroid in image space\n        y_indices, x_indices = np.where(mask)\n        cx = np.mean(x_indices)\n        cy = np.mean(y_indices)\n        z = np.median(valid_depths)\n\n        # Project to 3D (assuming pinhole camera)\n        fx, fy = 525.0, 525.0  # Focal lengths\n        x = (cx - 320) * z / fx\n        y = (cy - 240) * z / fy\n\n        return [x, y, z]\n'})}),"\n",(0,s.jsx)(e.h3,{id:"speech-recognition",children:"Speech Recognition"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import whisper\nfrom transformers import pipeline\n\nclass SpeechNode(Node):\n    """Speech recognition and understanding."""\n\n    def __init__(self):\n        super().__init__(\'speech\')\n\n        # Load Whisper\n        self.whisper_model = whisper.load_model("base")\n\n        # Load LLM for command parsing\n        self.command_parser = pipeline(\n            "text-generation",\n            model="microsoft/phi-2"\n        )\n\n        # Audio subscriber\n        self.audio_sub = self.create_subscription(\n            AudioData, \'/microphone/audio\', self.audio_callback, 10\n        )\n\n        # Command publisher\n        self.command_pub = self.create_publisher(\n            TaskCommand, \'/task_command\', 10\n        )\n\n        self.audio_buffer = []\n\n    def audio_callback(self, msg):\n        """Process incoming audio."""\n        self.audio_buffer.extend(msg.data)\n\n        # Process every 3 seconds\n        if len(self.audio_buffer) >= 48000 * 3:  # 16kHz * 3s\n            audio = np.array(self.audio_buffer[:48000*3], dtype=np.float32) / 32768.0\n            self.audio_buffer = self.audio_buffer[48000*3:]\n\n            # Transcribe\n            result = self.whisper_model.transcribe(audio)\n            text = result["text"].strip()\n\n            if text:\n                self.get_logger().info(f"Heard: {text}")\n                self._parse_command(text)\n\n    def _parse_command(self, text: str):\n        """Parse natural language to robot command."""\n        prompt = f"""Parse this command into a robot action:\nCommand: "{text}"\n\nOutput JSON with fields:\n- command_type: "navigate", "pick", "place", or "speak"\n- target_object: object name or null\n- target_location: location name or null\n\nJSON:"""\n\n        response = self.command_parser(prompt, max_length=100)[0]["generated_text"]\n\n        try:\n            # Extract JSON from response\n            json_str = response.split("JSON:")[-1].strip()\n            command_data = json.loads(json_str)\n\n            # Publish command\n            msg = TaskCommand()\n            msg.header.stamp = self.get_clock().now().to_msg()\n            msg.command_type = command_data.get("command_type", "")\n            msg.target_object = command_data.get("target_object", "")\n            self.command_pub.publish(msg)\n\n        except json.JSONDecodeError:\n            self.get_logger().warn("Could not parse command")\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"64-planning-module",children:"6.4 Planning Module"}),"\n",(0,s.jsx)(e.h3,{id:"task-planner",children:"Task Planner"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from enum import Enum\nfrom dataclasses import dataclass\n\nclass TaskState(Enum):\n    IDLE = "idle"\n    NAVIGATING = "navigating"\n    APPROACHING = "approaching"\n    PICKING = "picking"\n    PLACING = "placing"\n    SPEAKING = "speaking"\n    ERROR = "error"\n\n@dataclass\nclass Task:\n    task_type: str\n    target: str\n    parameters: dict\n    priority: int = 0\n\nclass TaskPlanner(Node):\n    """High-level task planning for humanoid."""\n\n    def __init__(self):\n        super().__init__(\'task_planner\')\n\n        self.current_state = TaskState.IDLE\n        self.task_queue = []\n        self.current_task = None\n\n        # World model\n        self.known_objects = {}\n        self.known_locations = {\n            "kitchen": [2.0, 3.0, 0.0],\n            "living_room": [5.0, 1.0, 0.0],\n            "table": [3.0, 2.0, 0.8]\n        }\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            TaskCommand, \'/task_command\', self.command_callback, 10\n        )\n        self.detection_sub = self.create_subscription(\n            Detection3DArray, \'/detections\', self.detection_callback, 10\n        )\n\n        # Publishers\n        self.nav_goal_pub = self.create_publisher(\n            PoseStamped, \'/goal_pose\', 10\n        )\n        self.manipulation_goal_pub = self.create_publisher(\n            ManipulationGoal, \'/manipulation_goal\', 10\n        )\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Planning loop\n        self.timer = self.create_timer(0.1, self.planning_loop)\n\n    def command_callback(self, msg):\n        """Receive and queue new task."""\n        task = Task(\n            task_type=msg.command_type,\n            target=msg.target_object,\n            parameters={}\n        )\n        self.task_queue.append(task)\n        self.get_logger().info(f"Queued task: {task.task_type} -> {task.target}")\n\n    def planning_loop(self):\n        """Main planning state machine."""\n        if self.current_state == TaskState.IDLE:\n            if self.task_queue:\n                self.current_task = self.task_queue.pop(0)\n                self._start_task(self.current_task)\n\n        elif self.current_state == TaskState.NAVIGATING:\n            self._check_navigation_status()\n\n        elif self.current_state == TaskState.PICKING:\n            self._check_manipulation_status()\n\n    def _start_task(self, task: Task):\n        """Start executing a task."""\n        if task.task_type == "navigate":\n            self._start_navigation(task.target)\n        elif task.task_type == "pick":\n            self._start_pick(task.target)\n        elif task.task_type == "place":\n            self._start_place(task.target)\n\n    def _start_navigation(self, target: str):\n        """Start navigation to target location."""\n        if target in self.known_locations:\n            pose = self.known_locations[target]\n\n            goal = NavigateToPose.Goal()\n            goal.pose.header.frame_id = "map"\n            goal.pose.pose.position.x = pose[0]\n            goal.pose.pose.position.y = pose[1]\n            goal.pose.pose.orientation.w = 1.0\n\n            self.nav_client.send_goal_async(goal)\n            self.current_state = TaskState.NAVIGATING\n            self.get_logger().info(f"Navigating to {target}")\n\n    def _start_pick(self, target: str):\n        """Start picking an object."""\n        if target in self.known_objects:\n            obj = self.known_objects[target]\n\n            goal = ManipulationGoal()\n            goal.action = "pick"\n            goal.target_pose = obj.pose\n            goal.object_id = target\n\n            self.manipulation_goal_pub.publish(goal)\n            self.current_state = TaskState.PICKING\n            self.get_logger().info(f"Picking {target}")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"motion-planning-with-moveit2",children:"Motion Planning with MoveIt2"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from moveit_msgs.msg import MoveGroupAction, Constraints\nfrom moveit_msgs.srv import GetPositionIK\n\nclass ManipulationController(Node):\n    """Manipulation control using MoveIt2."""\n\n    def __init__(self):\n        super().__init__(\'manipulation\')\n\n        # MoveIt interface\n        self.move_group = MoveGroupInterface(\n            group_name="arm",\n            ns="",\n            robot_description="robot_description"\n        )\n\n        # IK service\n        self.ik_client = self.create_client(\n            GetPositionIK, \'/compute_ik\'\n        )\n\n        # VLA model for learned manipulation\n        self.vla_model = OpenVLA.load_pretrained("openvla-humanoid")\n        self.vla_model.eval()\n\n        # Goal subscriber\n        self.goal_sub = self.create_subscription(\n            ManipulationGoal, \'/manipulation_goal\',\n            self.goal_callback, 10\n        )\n\n        # Joint command publisher\n        self.joint_pub = self.create_publisher(\n            JointTrajectory, \'/arm_controller/command\', 10\n        )\n\n    def goal_callback(self, msg):\n        """Handle manipulation goal."""\n        if msg.action == "pick":\n            self._execute_pick(msg.target_pose, msg.object_id)\n        elif msg.action == "place":\n            self._execute_place(msg.target_pose)\n\n    def _execute_pick(self, target_pose, object_id):\n        """Execute pick action using VLA."""\n        # Get current images\n        images = self._get_camera_images()\n\n        # Generate instruction\n        instruction = f"Pick up the {object_id}"\n\n        # Get VLA actions\n        actions = []\n        for step in range(20):  # Max 20 steps\n            action = self.vla_model.predict(images, instruction)\n            actions.append(action)\n\n            # Execute action\n            self._send_joint_command(action[:6])\n\n            # Check grasp\n            if self._check_grasp():\n                break\n\n            # Update images\n            time.sleep(0.1)\n            images = self._get_camera_images()\n\n        self.get_logger().info(f"Pick completed in {len(actions)} steps")\n\n    def _send_joint_command(self, positions):\n        """Send joint position command."""\n        msg = JointTrajectory()\n        msg.joint_names = [\n            "shoulder_pan", "shoulder_lift", "elbow",\n            "wrist_1", "wrist_2", "wrist_3"\n        ]\n\n        point = JointTrajectoryPoint()\n        point.positions = positions.tolist()\n        point.time_from_start.sec = 0\n        point.time_from_start.nanosec = 100000000  # 100ms\n\n        msg.points = [point]\n        self.joint_pub.publish(msg)\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"65-control-module",children:"6.5 Control Module"}),"\n",(0,s.jsx)(e.h3,{id:"whole-body-control",children:"Whole-Body Control"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy.linalg import solve\n\nclass WholeBodyController:\n    """Whole-body control for humanoid locomotion and manipulation."""\n\n    def __init__(self, robot_model):\n        self.model = robot_model\n        self.num_joints = robot_model.num_joints\n\n        # Control gains\n        self.kp_balance = 100.0\n        self.kd_balance = 20.0\n        self.kp_task = 50.0\n        self.kd_task = 10.0\n\n    def compute_control(\n        self,\n        q: np.ndarray,           # Current joint positions\n        dq: np.ndarray,          # Current joint velocities\n        task_target: np.ndarray, # Desired end-effector pose\n        com_target: np.ndarray,  # Desired center of mass\n    ) -> np.ndarray:\n        """Compute joint torques for whole-body control."""\n\n        # Forward kinematics\n        ee_pose = self.model.forward_kinematics(q)\n        com = self.model.center_of_mass(q)\n\n        # Jacobians\n        J_ee = self.model.end_effector_jacobian(q)\n        J_com = self.model.com_jacobian(q)\n\n        # Task-space errors\n        ee_error = task_target - ee_pose\n        com_error = com_target - com\n\n        # Task-space velocities\n        ee_vel = J_ee @ dq\n        com_vel = J_com @ dq\n\n        # Desired task accelerations (PD control)\n        ddx_ee = self.kp_task * ee_error - self.kd_task * ee_vel\n        ddx_com = self.kp_balance * com_error - self.kd_balance * com_vel\n\n        # Stack tasks (COM has higher priority)\n        J_stack = np.vstack([J_com, J_ee])\n        ddx_stack = np.hstack([ddx_com, ddx_ee])\n\n        # Solve for joint accelerations (least squares)\n        ddq = np.linalg.lstsq(J_stack, ddx_stack, rcond=None)[0]\n\n        # Inverse dynamics to get torques\n        tau = self.model.inverse_dynamics(q, dq, ddq)\n\n        return tau\n\n    def balance_control(\n        self,\n        q: np.ndarray,\n        dq: np.ndarray,\n        imu_orientation: np.ndarray,\n        foot_contacts: list[bool]\n    ) -> np.ndarray:\n        """Compute torques for maintaining balance."""\n\n        # Desired upright orientation\n        target_orientation = np.array([1, 0, 0, 0])  # Identity quaternion\n\n        # Orientation error (quaternion difference)\n        orientation_error = self._quaternion_error(\n            target_orientation, imu_orientation\n        )\n\n        # Angular velocity (from IMU)\n        angular_vel = self.model.angular_velocity(q, dq)\n\n        # Compute corrective torques\n        tau_balance = (\n            self.kp_balance * orientation_error\n            - self.kd_balance * angular_vel\n        )\n\n        # Map to joint torques based on contact state\n        if all(foot_contacts):  # Double support\n            tau = self._double_support_mapping(tau_balance, q)\n        elif foot_contacts[0]:  # Left foot only\n            tau = self._single_support_mapping(tau_balance, q, "left")\n        elif foot_contacts[1]:  # Right foot only\n            tau = self._single_support_mapping(tau_balance, q, "right")\n        else:  # Flight phase\n            tau = np.zeros(self.num_joints)\n\n        return tau\n'})}),"\n",(0,s.jsx)(e.h3,{id:"locomotion-controller",children:"Locomotion Controller"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class LocomotionController(Node):\n    """Bipedal locomotion controller."""\n\n    def __init__(self):\n        super().__init__(\'locomotion\')\n\n        # Load trained policy\n        self.policy = torch.jit.load("locomotion_policy.pt")\n        self.policy.eval()\n\n        # State estimation\n        self.state_estimator = StateEstimator()\n\n        # Subscribers\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10\n        )\n        self.joint_state_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_state_callback, 10\n        )\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'/cmd_vel\', self.cmd_vel_callback, 10\n        )\n\n        # Publisher\n        self.joint_cmd_pub = self.create_publisher(\n            JointTrajectory, \'/joint_trajectory_controller/command\', 10\n        )\n\n        # Control loop at 100 Hz\n        self.timer = self.create_timer(0.01, self.control_loop)\n\n        self.target_velocity = np.zeros(3)\n        self.current_state = None\n\n    def control_loop(self):\n        """Main control loop."""\n        if self.current_state is None:\n            return\n\n        # Build observation\n        obs = self._build_observation()\n\n        # Get action from policy\n        with torch.no_grad():\n            obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)\n            action = self.policy(obs_tensor).numpy().squeeze()\n\n        # Convert to joint commands\n        joint_targets = self._action_to_joints(action)\n\n        # Publish command\n        self._publish_joint_command(joint_targets)\n\n    def _build_observation(self) -> np.ndarray:\n        """Build observation vector for policy."""\n        return np.concatenate([\n            self.current_state.joint_positions,\n            self.current_state.joint_velocities,\n            self.current_state.base_orientation,\n            self.current_state.base_angular_velocity,\n            self.target_velocity,\n            np.sin(self.phase),  # Gait phase\n            np.cos(self.phase),\n        ])\n\n    def _action_to_joints(self, action: np.ndarray) -> np.ndarray:\n        """Convert policy action to joint positions."""\n        # Action is delta from default pose\n        default_pose = np.array([\n            0, 0.5, -1.0, 0, 0.5, -1.0,  # Legs (6 DOF each)\n            0, 0, 0, 0, 0, 0              # Arms\n        ])\n        return default_pose + action * 0.1  # Scale factor\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"66-system-integration",children:"6.6 System Integration"}),"\n",(0,s.jsx)(e.h3,{id:"state-machine-coordinator",children:"State Machine Coordinator"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from transitions import Machine\n\nclass HumanoidCoordinator(Node):\n    \"\"\"Central coordinator using state machine.\"\"\"\n\n    STATES = [\n        'idle', 'listening', 'planning',\n        'navigating', 'manipulating', 'speaking', 'error'\n    ]\n\n    def __init__(self):\n        super().__init__('coordinator')\n\n        # Initialize state machine\n        self.machine = Machine(\n            model=self,\n            states=self.STATES,\n            initial='idle'\n        )\n\n        # Define transitions\n        self.machine.add_transition('hear_command', 'idle', 'listening')\n        self.machine.add_transition('understand', 'listening', 'planning')\n        self.machine.add_transition('plan_ready', 'planning', 'navigating')\n        self.machine.add_transition('arrived', 'navigating', 'manipulating')\n        self.machine.add_transition('done', 'manipulating', 'speaking')\n        self.machine.add_transition('finished', 'speaking', 'idle')\n        self.machine.add_transition('fail', '*', 'error')\n        self.machine.add_transition('recover', 'error', 'idle')\n\n        # ROS interfaces\n        self.setup_ros_interfaces()\n\n        # Heartbeat\n        self.timer = self.create_timer(0.1, self.heartbeat)\n\n    def setup_ros_interfaces(self):\n        \"\"\"Set up all ROS subscribers and publishers.\"\"\"\n        # Status publisher\n        self.status_pub = self.create_publisher(\n            HumanoidState, '/humanoid_status', 10\n        )\n\n        # Monitor all subsystems\n        self.nav_status_sub = self.create_subscription(\n            GoalStatus, '/navigate_to_pose/_action/status',\n            self.nav_status_callback, 10\n        )\n        self.manipulation_status_sub = self.create_subscription(\n            String, '/manipulation_status',\n            self.manipulation_status_callback, 10\n        )\n\n    def heartbeat(self):\n        \"\"\"Publish system status.\"\"\"\n        msg = HumanoidState()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.current_state = self.state\n        msg.current_task = str(self.current_task) if hasattr(self, 'current_task') else \"\"\n        self.status_pub.publish(msg)\n\n    def on_enter_error(self):\n        \"\"\"Handle error state.\"\"\"\n        self.get_logger().error(\"System entered error state\")\n        # Stop all motion\n        self._emergency_stop()\n        # Attempt recovery after delay\n        self.create_timer(5.0, lambda: self.recover(), oneshot=True)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"system-launch",children:"System Launch"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# launch/full_system.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, GroupAction\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import PushRosNamespace\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Hardware drivers\n        GroupAction([\n            PushRosNamespace('hardware'),\n            IncludeLaunchDescription(\n                PythonLaunchDescriptionSource('drivers.launch.py')\n            ),\n        ]),\n\n        # Perception\n        GroupAction([\n            PushRosNamespace('perception'),\n            IncludeLaunchDescription(\n                PythonLaunchDescriptionSource('perception.launch.py')\n            ),\n        ]),\n\n        # Navigation\n        GroupAction([\n            PushRosNamespace('navigation'),\n            IncludeLaunchDescription(\n                PythonLaunchDescriptionSource('nav2_bringup.launch.py'),\n                launch_arguments={\n                    'map': '/maps/lab.yaml',\n                    'params_file': '/config/nav2_params.yaml'\n                }.items()\n            ),\n        ]),\n\n        # Control\n        GroupAction([\n            PushRosNamespace('control'),\n            IncludeLaunchDescription(\n                PythonLaunchDescriptionSource('control.launch.py')\n            ),\n        ]),\n\n        # Coordinator\n        Node(\n            package='humanoid_coordination',\n            executable='coordinator',\n            name='coordinator',\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"67-testing-and-evaluation",children:"6.7 Testing and Evaluation"}),"\n",(0,s.jsx)(e.h3,{id:"unit-tests",children:"Unit Tests"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import pytest\nimport rclpy\nfrom humanoid_perception.perception_node import PerceptionNode\n\nclass TestPerception:\n    """Test perception module."""\n\n    @pytest.fixture\n    def node(self):\n        rclpy.init()\n        node = PerceptionNode()\n        yield node\n        node.destroy_node()\n        rclpy.shutdown()\n\n    def test_object_detection(self, node):\n        """Test object detection accuracy."""\n        test_image = load_test_image("test_scene.png")\n        detections = node.detect_objects(test_image)\n\n        assert len(detections) > 0\n        assert all(d.confidence > 0.5 for d in detections)\n\n    def test_pose_estimation(self, node):\n        """Test 3D pose estimation."""\n        test_image = load_test_image("test_object.png")\n        test_depth = load_test_depth("test_depth.png")\n\n        pose = node.estimate_pose(test_image, test_depth)\n\n        # Should be within 5cm of ground truth\n        assert np.linalg.norm(pose - GROUND_TRUTH_POSE) < 0.05\n\nclass TestLocomotion:\n    """Test locomotion controller."""\n\n    def test_balance_recovery(self, simulation):\n        """Test recovery from push disturbance."""\n        # Apply push\n        simulation.apply_force([0, 50, 0], duration=0.1)\n\n        # Wait for recovery\n        simulation.step(seconds=3.0)\n\n        # Check upright\n        orientation = simulation.get_robot_orientation()\n        tilt = quaternion_to_euler(orientation)[1]  # Pitch\n        assert abs(tilt) < 0.1  # Less than 6 degrees\n\n    def test_walking_speed(self, simulation):\n        """Test walking velocity tracking."""\n        target_vel = 0.5  # m/s\n        simulation.set_cmd_vel(target_vel, 0, 0)\n\n        simulation.step(seconds=10.0)\n\n        actual_vel = simulation.get_robot_velocity()[0]\n        assert abs(actual_vel - target_vel) < 0.1\n'})}),"\n",(0,s.jsx)(e.h3,{id:"integration-tests",children:"Integration Tests"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class TestSystemIntegration:\n    """End-to-end system tests."""\n\n    def test_voice_to_action(self, system):\n        """Test complete voice command pipeline."""\n        # Send voice command\n        system.send_audio("Pick up the red cup from the table")\n\n        # Wait for execution\n        system.wait_for_state("idle", timeout=60)\n\n        # Verify cup was picked\n        assert system.gripper_holding_object()\n        assert system.last_object_picked == "red_cup"\n\n    def test_navigation_and_manipulation(self, system):\n        """Test combined navigation and manipulation."""\n        # Start at origin\n        system.reset_robot_pose([0, 0, 0])\n\n        # Command to fetch object\n        system.execute_command("Go to the kitchen and get a water bottle")\n\n        # Should navigate to kitchen\n        system.wait_for_state("manipulating", timeout=30)\n        assert np.linalg.norm(system.robot_pose[:2] - KITCHEN_POS[:2]) < 0.5\n\n        # Should pick up bottle\n        system.wait_for_state("idle", timeout=30)\n        assert system.gripper_holding_object()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Metric"}),(0,s.jsx)(e.th,{children:"Target"}),(0,s.jsx)(e.th,{children:"Measurement Method"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Task success rate"}),(0,s.jsx)(e.td,{children:">90%"}),(0,s.jsx)(e.td,{children:"100 trial average"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Navigation accuracy"}),(0,s.jsx)(e.td,{children:"Less than 10cm"}),(0,s.jsx)(e.td,{children:"RMS error to goal"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Pick success"}),(0,s.jsx)(e.td,{children:">85%"}),(0,s.jsx)(e.td,{children:"Grasp completion"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Response latency"}),(0,s.jsx)(e.td,{children:"Less than 500ms"}),(0,s.jsx)(e.td,{children:"Command to motion"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Battery life"}),(0,s.jsx)(e.td,{children:">2 hours"}),(0,s.jsx)(e.td,{children:"Full operation time"})]})]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"68-documentation-and-presentation",children:"6.8 Documentation and Presentation"}),"\n",(0,s.jsx)(e.h3,{id:"project-documentation",children:"Project Documentation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-markdown",children:"# Humanoid Robot System Documentation\n\n## System Overview\n[Architecture diagram and description]\n\n## Installation\n```bash\n# Clone repository\ngit clone https://github.com/your-org/humanoid-system.git\n\n# Install dependencies\ncd humanoid-system\nrosdep install --from-paths src --ignore-src -r -y\n\n# Build\ncolcon build --symlink-install\n"})}),"\n",(0,s.jsx)(e.h2,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsx)(e.p,{children:"[Parameter files and tuning guide]"}),"\n",(0,s.jsx)(e.h2,{id:"operation-manual",children:"Operation Manual"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Power on sequence"}),"\n",(0,s.jsx)(e.li,{children:"Calibration procedure"}),"\n",(0,s.jsx)(e.li,{children:"Starting the system"}),"\n",(0,s.jsx)(e.li,{children:"Voice commands reference"}),"\n",(0,s.jsx)(e.li,{children:"Emergency stop procedure"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(e.p,{children:"[Common issues and solutions]"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\n### Demo Presentation\n\n1. **Introduction** (2 min)\n   - Problem statement\n   - System overview\n\n2. **Technical Deep-Dive** (5 min)\n   - Architecture walkthrough\n   - Key innovations\n\n3. **Live Demo** (10 min)\n   - Voice command demonstration\n   - Navigation and manipulation\n   - Error recovery\n\n4. **Results** (3 min)\n   - Performance metrics\n   - Comparison to baselines\n\n5. **Future Work** (2 min)\n   - Planned improvements\n   - Research directions\n\n---\n\n## Chapter Summary\n\n- Capstone integrates perception, planning, and control\n- State machine coordinates all subsystems\n- VLA models enable natural language control\n- Thorough testing ensures reliability\n- Documentation enables reproducibility\n\n---\n\n## Final Project Checklist\n\n- [ ] Perception module detects >10 object classes\n- [ ] Navigation works in 10x10m environment\n- [ ] Manipulation succeeds >80% for tabletop objects\n- [ ] Voice commands recognized with >95% accuracy\n- [ ] System responds within 500ms\n- [ ] Recovers from push disturbances\n- [ ] Runs for >1 hour on battery\n- [ ] Documentation complete\n- [ ] Video demo recorded\n- [ ] Code repository organized\n\n---\n\n## Congratulations!\n\nYou have completed the Physical AI & Humanoid Robotics curriculum. You now have the skills to:\n\n- Design and build humanoid robot software systems\n- Integrate perception, planning, and control\n- Train and deploy neural network policies\n- Use modern VLA models for intelligent control\n- Test, document, and present robotics projects\n\n**Continue your journey:**\n- Contribute to open-source humanoid projects\n- Explore latest research in embodied AI\n- Build your own humanoid robot!\n"})})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>i,x:()=>r});var o=t(6540);const s={},a=o.createContext(s);function i(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);